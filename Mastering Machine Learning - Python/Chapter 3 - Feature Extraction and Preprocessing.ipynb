{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "\n",
    "import sklearn\n",
    "from pandas import pivot_table, read_clipboard\n",
    "import os, sys, traceback\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "onehot_encoder = DictVectorizer()\n",
    "instances = [\n",
    "{'city': 'New York'},\n",
    "{'city': 'San Francisco'},\n",
    "{'city': 'Chapel Hill'}]\n",
    "print onehot_encoder.fit_transform(instances).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0 1 0 0 1]\n",
      " [0 1 1 1 0 1 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 1 0 0]]\n",
      "{u'duke': 2, u'basketball': 1, u'lost': 5, u'played': 6, u'in': 4, u'game': 3, u'sandwich': 7, u'unc': 9, u'ate': 0, u'the': 8}\n",
      "UNC played Duke in basketball = [[0 1 1 0 1 0 1 0 0 1]]\n",
      "Duke lost the basketball game = [[0 1 1 1 0 1 0 0 1 0]]\n",
      "I ate a sandwich = [[1 0 0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#corpus is a list of text documents\n",
    "corpus = ['UNC played Duke in basketball', 'Duke lost the basketball game', 'I ate a sandwich']\n",
    "#Tokenizes the text into strings with at least 2 character lengths and then counts them \n",
    "vectorizer = CountVectorizer()\n",
    "dense_corp = vectorizer.fit_transform(corpus).todense()\n",
    "print dense_corp\n",
    "print vectorizer.vocabulary_\n",
    "for i, val in enumerate(corpus):\n",
    "    print val + \" = \" + str(dense_corp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st document: UNC played Duke in basketball\n",
      "2nd document: Duke lost the basketball game\n",
      "3rd document: I ate a sandwich\n",
      "[[0, 1, 1, 0, 1, 0, 1, 0, 0, 1], [0, 1, 1, 1, 0, 1, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 1, 0, 0]]\n",
      "Distance between 1st and 2nd documents: [[ 2.44948974]]\n",
      "Distance between 1st and 3rd documents: [[ 2.64575131]]\n",
      "Distance between 2nd and 3rd documents: [[ 2.64575131]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "#euclidean distances measures the similarity between two texts\n",
    "counts = [x for x in dense_corp.astype(int).tolist()]\n",
    "print '1st document:', corpus[0]\n",
    "print '2nd document:', corpus[1]\n",
    "print '3rd document:', corpus[2]\n",
    "print counts\n",
    "print 'Distance between 1st and 2nd documents:', euclidean_distances(counts[0], counts[1])\n",
    "print 'Distance between 1st and 3rd documents:', euclidean_distances(counts[0], counts[2])\n",
    "print 'Distance between 2nd and 3rd documents:', euclidean_distances(counts[1], counts[2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "This is high dimensional vectors though as each vector in each sentence has it's own representation. using this method means we will have many sparse vectors (lots of non zeros) and curse of dimensonality/Hughes Effect where we need to have more training which each combination of feature values to ensure the algorithm can generalize the results enough and not overfit the noise.\n",
    "\n",
    "A few basic strategy to reduce dimensions are stop word filtering, stemming/lemmatization (condense jumps and jump to one word). Lemma looks at the root words based on a dictionary of words. Stemma looks at words and removes patterns that it believes are derivations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 1 0 1]\n",
      " [0 1 1 1 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 0]]\n",
      "{u'duke': 2, u'basketball': 1, u'lost': 4, u'played': 5, u'game': 3, u'sandwich': 6, u'unc': 7, u'ate': 0}\n",
      "UNC played Duke in basketball = [[0 1 1 0 0 1 0 1]]\n",
      "Duke lost the basketball game = [[0 1 1 1 1 0 0 0]]\n",
      "I ate a sandwich = [[1 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# STOP WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#corpus is a list of text documents\n",
    "corpus = ['UNC played Duke in basketball', 'Duke lost the basketball game', 'I ate a sandwich']\n",
    "#Tokenizes the text into strings with at least 2 character lengths and then counts them \n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "dense_corp = vectorizer.fit_transform(corpus).todense()\n",
    "print dense_corp\n",
    "print vectorizer.vocabulary_\n",
    "for i, val in enumerate(corpus):\n",
    "    print val + \" = \" + str(dense_corp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://nltk.github.com/nltk_data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stemming/Lemmatization\n",
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather\n",
      "gathering\n",
      "gather\n",
      "Stemmed: [[u'He', u'ate', u'the', u'sandwich'], [u'Everi', u'sandwich', u'wa', u'eaten', u'by', u'him']]\n",
      "Lemmatized: [['He', u'eat', 'the', u'sandwich'], ['Every', 'sandwich', u'be', u'eat', 'by', 'him']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print lemmatizer.lemmatize('gathering', 'v')\n",
    "print lemmatizer.lemmatize('gathering', 'n')\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print stemmer.stem('gathering')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "wordnet_tags = ['n', 'v']\n",
    "corpus = ['He ate the sandwiches','Every sandwich was eaten by him']\n",
    "stemmer = PorterStemmer()\n",
    "print 'Stemmed:', [[stemmer.stem(token) for token in word_tokenize(document)] for document in corpus]\n",
    "\n",
    "def lemmatize(token, tag):\n",
    "    if tag[0].lower() in ['n', 'v']:\n",
    "        return lemmatizer.lemmatize(token, tag[0].lower())\n",
    "    return token\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tagged_corpus = [pos_tag(word_tokenize(document)) for document in corpus]\n",
    "print 'Lemmatized:', [[lemmatize(token, tag) for token, tag in document] for document in tagged_corpus]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "We might want to also look at the counts of words within documents to help show the similarity between two documents. Bt we would need to be sure to transform based on the length of the documents (100000 length blog post vs a 100 word one should be compared on a similar scale). This is where the TfdfTransformer comes in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.79085927  0.          0.39542964  0.          0.46709423  0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.38537163  0.65249088\n",
      "   0.65249088]\n",
      " [ 0.          0.65249088  0.          0.65249088  0.38537163  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#TRANSFORMED\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = ['The dog ate a sandwich and I ate a sandwich', 'The wizard transfigured a sandwich', 'sandwich is the capital of germany']\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "sandwich_copr_vectors = vectorizer.fit_transform(corpus).todense()\n",
    "print sandwich_copr_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st document: The dog ate a sandwich and I ate a sandwich\n",
      "2nd document: The wizard transfigured a sandwich\n",
      "[[0.7908592715238688, 0.0, 0.3954296357619344, 0.0, 0.467094225832347, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.3853716274664007, 0.652490884512534, 0.652490884512534], [0.0, 0.652490884512534, 0.0, 0.652490884512534, 0.3853716274664007, 0.0, 0.0]]\n",
      "Distance between 1st and 2nd documents: [[ 1.28062105]]\n",
      "Distance between 1st and 3rd documents: [[ 1.28062105]]\n",
      "Distance between 2nd and 3rd documents: [[ 1.30498177]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "#euclidean distances measures the similarity between two texts\n",
    "counts = [x for x in sandwich_copr_vectors.astype(float).tolist()]\n",
    "print '1st document:', corpus[0]\n",
    "print '2nd document:', corpus[1]\n",
    "print counts\n",
    "print 'Distance between 1st and 2nd documents:', euclidean_distances(counts[0], counts[1])\n",
    "print 'Distance between 1st and 3rd documents:', euclidean_distances(counts[0], counts[2])\n",
    "print 'Distance between 2nd and 3rd documents:', euclidean_distances(counts[1], counts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Hashing is a way of performing feature vectorizors effeciently. Rather than investigating both documents in their entirerty and then counting the vectors, hasing does this in place by incrementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.feature_extraction.text import HashingVectorizer\n",
    ">>> corpus = ['the', 'ate', 'bacon', 'cat']\n",
    "#n_features is optional\n",
    ">>> vectorizer = HashingVectorizer(n_features=6)\n",
    ">>> print vectorizer.transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit: 0\n",
      "[[  0.   0.   5.  13.   9.   1.   0.   0.]\n",
      " [  0.   0.  13.  15.  10.  15.   5.   0.]\n",
      " [  0.   3.  15.   2.   0.  11.   8.   0.]\n",
      " [  0.   4.  12.   0.   0.   8.   8.   0.]\n",
      " [  0.   5.   8.   0.   0.   9.   8.   0.]\n",
      " [  0.   4.  11.   0.   1.  12.   7.   0.]\n",
      " [  0.   2.  14.   5.  10.  12.   0.   0.]\n",
      " [  0.   0.   6.  13.  10.   0.   0.   0.]]\n",
      "Feature vector:\n",
      "[[  0.   0.   5.  13.   9.   1.   0.   0.   0.   0.  13.  15.  10.  15.\n",
      "    5.   0.   0.   3.  15.   2.   0.  11.   8.   0.   0.   4.  12.   0.\n",
      "    0.   8.   8.   0.   0.   5.   8.   0.   0.   9.   8.   0.   0.   4.\n",
      "   11.   0.   1.  12.   7.   0.   0.   2.  14.   5.  10.  12.   0.   0.\n",
      "    0.   0.   6.  13.  10.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Extract features from images\n",
    ">>> from sklearn import datasets\n",
    ">>> digits = datasets.load_digits()\n",
    ">>> print 'Digit:', digits.target[0]\n",
    ">>> print digits.images[0]\n",
    ">>> print 'Feature vector:\\n', digits.images[0].reshape(-1, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using points of interest\n",
    ">>> import numpy as nps\n",
    ">>> from skimage.feature import corner_harris, corner_peaks\n",
    ">>> from skimage.color import rgb2gray\n",
    ">>> import matplotlib.pyplot as plt\n",
    ">>> import skimage.io as io\n",
    ">>> from skimage.exposure import equalize_hist\n",
    ">>> def show_corners(corners, image):\n",
    "    >>> fig = plt.figure()\n",
    "    >>> plt.gray()\n",
    "    >>> plt.imshow(image)\n",
    "    >>> y_corner, x_corner = zip(*corners)\n",
    "    >>> plt.plot(x_corner, y_corner, 'or')\n",
    "    >>> plt.xlim(0, image.shape[1])\n",
    "    >>> plt.ylim(image.shape[0], 0)\n",
    "    >>> fig.set_size_inches(np.array(fig.get_size_inches()) * 1.5)\n",
    "    >>> plt.show()\n",
    "    \n",
    ">>> mandrill = io.imread('/home/gavin/PycharmProjects/masteringmachine-\n",
    "learning/ch4/img/mandrill.png')\n",
    ">>> mandrill = equalize_hist(rgb2gray(mandrill))\n",
    ">>> corners = corner_peaks(corner_harris(mandrill), min_distance=2)\n",
    ">>> show_corners(corners, mandrill)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Preprocessing normalizes the x values so they are normally distributed about the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.70710678 -1.38873015  0.52489066  0.59299945 -1.35873244]\n",
      " [ 0.         -0.70710678  0.46291005  0.87481777  0.81537425  1.01904933]\n",
      " [ 0.          1.41421356  0.9258201  -1.39970842 -1.4083737   0.33968311]]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn import preprocessing\n",
    ">>> import numpy as np\n",
    ">>> X = np.array([\n",
    ">>> [0., 0., 5., 13., 9., 1.],\n",
    ">>> [0., 0., 13., 15., 10., 15.],\n",
    ">>> [0., 3., 15., 2., 0., 11.]\n",
    ">>> ])\n",
    ">>> print preprocessing.scale(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
