{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: Maximize Profit from these members. \n",
    "### Where to go from here? - questions? observations? directions?\n",
    "#### • emails.tsv: a tab-separated file of all emails sent to a subset of our members from the JobsRadar brand in September\n",
    "Column1: email_id\n",
    "Column2: email_send_time (EST)\n",
    "Column3: email_type\n",
    "\tT plus 1 is a campaign sent 1 day after a member joins\n",
    "\tT plus N are campaigns sent N days after a member joins\n",
    "\tTransactional Forgot Password Email\n",
    "\tTransactional JR Welcome Email\n",
    "Column4: email_variant\n",
    "\tFirst Part:\n",
    "\t\tfixed_keyword_cloud = keyword cloud email (See samples)\n",
    "\t\tjob_alert = custom job listings (see samples)\n",
    "\t2nd part:\n",
    "\t\ttplusN helps with time since joined (joined N days ago)\n",
    "\t\tage22+ goes to people age 22+ (includes anyone 22-999, meaning everyone except under 22),\n",
    "\t\tage35+ likewise\n",
    "\t\t1opened goes to people that opened at least once before.\n",
    "Column5: member_id\n",
    "\n",
    "job_alert_s1_v1 is a specific “version” of a “job alert” type of email we send.\n",
    "tplus2 means this message was sent to members that joined our site 2 days ago\n",
    "tplus201 means this message was sent to members that joined our site 201 days ago\n",
    "age22+ means it was sent to people that were at least 22 years old.\n",
    "1opened means it was sent to people that had opened at least 1 message from us in the past (“Openers”)\n",
    "\n",
    "#### • email_responses.tsv: a tab-separated file of open, click, and unsubscribe events that resulted from those emails\n",
    "#### • members.tsv: a tab-separated file of information about the members to whom those emails were sent\n",
    "#### • *.png: sample images of each of the email variants sent in this set (as identified by either the \"variant\" or \"campaign\" column in emails.tsv)\n",
    "#### • *.eml: Outlook export files of same\n",
    "\n",
    "#### A) we make 0.12 per click event \n",
    "#### B) it costs us 0.40 for every 1,000 emails sent.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import some useful python modules\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine, text\n",
    "import nltk\n",
    "\n",
    "def sqldf(query, sql_engine):\n",
    "    query_text = text(query)\n",
    "    results = pd.read_sql_query(query_text, con=sql_engine)\n",
    "    return results\n",
    "\n",
    "def sqlraw():\n",
    "    conn = sqlite3.connect('jobcase.db')\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change working directory into folder with data, view what is available\n",
    "os.chdir(os.getcwd() + '\\data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parse data into Pandas DataFrames\n",
    "# emails\n",
    "emails = pd.read_table('emails.tsv', header=None)\n",
    "emails.columns = ['email_id','timestamp','email_type','email_variant','member_id']\n",
    "emails.email_variant = emails.email_variant.apply(lambda x: str(x).replace('\\\\N', 'NONE')) \n",
    "emails['email_variant_first_part'] = emails['email_variant'].apply(lambda x: x.split(':')[0] if not pd.isnull(x) else np.nan)\n",
    "emails['email_variant_second_part'] = emails['email_variant'].apply(lambda x: x.split(':')[1] if not pd.isnull(x) and len(x.split(':')) > 1 else np.nan)\n",
    "emails.member_id = emails.member_id.astype(str)\n",
    "\n",
    "# email responses\n",
    "email_responses = pd.read_table('email_responses.tsv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52344.12\n",
      "3843.1236\n",
      "48500.9964\n"
     ]
    }
   ],
   "source": [
    "# Calculating the profit based on the amounts identified ($0.4/1000 and $0.12/email)\n",
    "total_cost = len(emails.email_id) * 0.4 / 1000\n",
    "gross_revenue = email_responses['action'].value_counts().click * 0.12\n",
    "profit = gross_revenue - total_cost\n",
    "print gross_revenue\n",
    "print total_cost\n",
    "print profit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this dataset, we can see that we have sent 9,607,809 emails. Sending these emails cost \\$0.4/1000 emails for a total cost of \\$3,843.12. Looking at the email responses we can see that there are 436,201 'click' events. Each click event earns Jobcase earns \\$0.12/click. This means that based on this dataset we have made \\$52,344.12 in revenue. Our profit on this dataset is then \\$48,501."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There are some issues with the members file (varying apparent row lengths, likely due to a separator issue) \n",
    "# so we will read that directly and try to identify where the issues are. \n",
    "# After the data is in a usable form and then parse the data into a dataframe manually\n",
    "members_file = open('members.tsv','r')\n",
    "members_list = []\n",
    "members_list_raw = []\n",
    "for line in members_file.readlines():\n",
    "    members_list_raw.append(line)\n",
    "    members_list.append(line.replace('\\n','').split('\\t'))\n",
    "\n",
    "#print members_list[0]\n",
    "#members_list_raw[205009]\n",
    "\n",
    "# From investigating the above rows, we can see that the issue stems from additional tabs in the value fields\n",
    "# we therefore need to replace the invalid dditional tab values so the data can be formatted correctly\n",
    "clean_members_list = [\n",
    "    [unicode(col.decode(\"ascii\",\"ignore\")) \n",
    "        for col in member.replace('\\n','').replace('\\\\\\t','').replace('\\N','').replace(\"'\",\"\").replace('+','').split('\\t')\n",
    "    ] for member in members_list_raw\n",
    "]\n",
    "\n",
    "members = pd.DataFrame(data=clean_members_list[1:], \n",
    "                       columns=clean_members_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails_and_clicks = pd.merge(emails, email_responses[email_responses.action=='click'], how='left', on='email_id', suffixes=['_1', '_2'])\n",
    "emails_and_resp = pd.merge(emails_and_clicks\n",
    "                           , pd.DataFrame(data=email_responses.email_id.unique(),columns=['email_id'])\n",
    "                           , how='inner', on='email_id', suffixes=['_1', '_2'])\n",
    "emails_and_resp['action'] = emails_and_resp['action'].fillna('na')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails_and_members = pd.merge(emails_and_resp, members, how='inner', on='member_id', suffixes=['_1', '_2'])\n",
    "emails_and_members.keyword = emails_and_members.keyword.fillna('').apply(lambda x: x.lower().replace('jobs','').replace('job','').replace('www','').replace('.com',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#emails.to_sql('emails', engine)\n",
    "#email_responses.to_sql('email_responses', engine)\n",
    "#members.to_sql('members', engine, if_exists='replace')\n",
    "#emails_and_members.to_sql('emails_and_members', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=30, min_df=2, stop_words='english',tokenizer=tokenize)\n",
    "vX = vectorizer.fit_transform(emails_and_members.keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emails_and_members['variant_vals'] = emails_and_members.email_variant.fillna('none').apply(lambda x: x.lower().replace('_',' ').replace(':',' ').replace('+',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vectorizer_variant_vals = TfidfVectorizer(max_df=0.5, max_features=1000, min_df=2, stop_words='english', tokenizer=tokenize)\n",
    "vX_variants = vectorizer_variant_vals.fit_transform(emails_and_members.variant_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_id</th>\n",
       "      <th>timestamp_1</th>\n",
       "      <th>email_type</th>\n",
       "      <th>email_variant</th>\n",
       "      <th>member_id</th>\n",
       "      <th>email_variant_first_part</th>\n",
       "      <th>email_variant_second_part</th>\n",
       "      <th>timestamp_2</th>\n",
       "      <th>action</th>\n",
       "      <th>date</th>\n",
       "      <th>email_domain</th>\n",
       "      <th>first_name</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>degree_level</th>\n",
       "      <th>hs_or_ged_year</th>\n",
       "      <th>pcp_score</th>\n",
       "      <th>keyword</th>\n",
       "      <th>variant_vals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>205570076</td>\n",
       "      <td>2012-09-01 00:10:08</td>\n",
       "      <td>Transactional JR Welcome Email</td>\n",
       "      <td>account_login_info_s2_v1</td>\n",
       "      <td>14802260</td>\n",
       "      <td>account_login_info_s2_v1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-09-01 00:23:49</td>\n",
       "      <td>click</td>\n",
       "      <td>2012-09-01 01:02:03</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>michael</td>\n",
       "      <td>SPARTA</td>\n",
       "      <td>TN</td>\n",
       "      <td>38583</td>\n",
       "      <td>Some HS</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.10344</td>\n",
       "      <td>kroger</td>\n",
       "      <td>account login info s2 v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>205570123</td>\n",
       "      <td>2012-09-01 00:11:07</td>\n",
       "      <td>Transactional Forgot Password Email</td>\n",
       "      <td>nan</td>\n",
       "      <td>8450299</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-09-01 00:12:57</td>\n",
       "      <td>click</td>\n",
       "      <td>2011-07-13 21:57:47</td>\n",
       "      <td>hotmail.com</td>\n",
       "      <td>Maria</td>\n",
       "      <td>Chula vista</td>\n",
       "      <td>CA</td>\n",
       "      <td>91911</td>\n",
       "      <td>Associate</td>\n",
       "      <td>1979</td>\n",
       "      <td>0.320591</td>\n",
       "      <td>costco</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>205570320</td>\n",
       "      <td>2012-09-01 00:30:08</td>\n",
       "      <td>Transactional JR Welcome Email</td>\n",
       "      <td>account_login_info_s2_v1</td>\n",
       "      <td>14802278</td>\n",
       "      <td>account_login_info_s2_v1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-09-01 00:31:09</td>\n",
       "      <td>click</td>\n",
       "      <td>2012-09-01 01:24:50</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>Frank</td>\n",
       "      <td>FORT LAUDERDALE</td>\n",
       "      <td>FL</td>\n",
       "      <td>33301</td>\n",
       "      <td>Some College</td>\n",
       "      <td>1981</td>\n",
       "      <td>0.174869</td>\n",
       "      <td>fedex</td>\n",
       "      <td>account login info s2 v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>206014887</td>\n",
       "      <td>2012-09-02 04:00:08</td>\n",
       "      <td>T plus N</td>\n",
       "      <td>fixed_keyword_cloud_s1_v1:tplus1_age22+</td>\n",
       "      <td>14802278</td>\n",
       "      <td>fixed_keyword_cloud_s1_v1</td>\n",
       "      <td>tplus1_age22+</td>\n",
       "      <td>2012-09-02 15:35:01</td>\n",
       "      <td>click</td>\n",
       "      <td>2012-09-01 01:24:50</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>Frank</td>\n",
       "      <td>FORT LAUDERDALE</td>\n",
       "      <td>FL</td>\n",
       "      <td>33301</td>\n",
       "      <td>Some College</td>\n",
       "      <td>1981</td>\n",
       "      <td>0.174869</td>\n",
       "      <td>fedex</td>\n",
       "      <td>fixed keyword cloud s1 v1 tplus1 age22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>206384628</td>\n",
       "      <td>2012-09-03 04:00:07</td>\n",
       "      <td>T plus N</td>\n",
       "      <td>fixed_keyword_cloud_s1_v1:tplus2</td>\n",
       "      <td>14802278</td>\n",
       "      <td>fixed_keyword_cloud_s1_v1</td>\n",
       "      <td>tplus2</td>\n",
       "      <td>2012-09-03 05:18:37</td>\n",
       "      <td>click</td>\n",
       "      <td>2012-09-01 01:24:50</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>Frank</td>\n",
       "      <td>FORT LAUDERDALE</td>\n",
       "      <td>FL</td>\n",
       "      <td>33301</td>\n",
       "      <td>Some College</td>\n",
       "      <td>1981</td>\n",
       "      <td>0.174869</td>\n",
       "      <td>fedex</td>\n",
       "      <td>fixed keyword cloud s1 v1 tplus2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    email_id          timestamp_1                           email_type  \\\n",
       "0  205570076  2012-09-01 00:10:08       Transactional JR Welcome Email   \n",
       "1  205570123  2012-09-01 00:11:07  Transactional Forgot Password Email   \n",
       "2  205570320  2012-09-01 00:30:08       Transactional JR Welcome Email   \n",
       "3  206014887  2012-09-02 04:00:08                             T plus N   \n",
       "4  206384628  2012-09-03 04:00:07                             T plus N   \n",
       "\n",
       "                             email_variant member_id  \\\n",
       "0                 account_login_info_s2_v1  14802260   \n",
       "1                                      nan   8450299   \n",
       "2                 account_login_info_s2_v1  14802278   \n",
       "3  fixed_keyword_cloud_s1_v1:tplus1_age22+  14802278   \n",
       "4         fixed_keyword_cloud_s1_v1:tplus2  14802278   \n",
       "\n",
       "    email_variant_first_part email_variant_second_part          timestamp_2  \\\n",
       "0   account_login_info_s2_v1                       NaN  2012-09-01 00:23:49   \n",
       "1                        nan                       NaN  2012-09-01 00:12:57   \n",
       "2   account_login_info_s2_v1                       NaN  2012-09-01 00:31:09   \n",
       "3  fixed_keyword_cloud_s1_v1             tplus1_age22+  2012-09-02 15:35:01   \n",
       "4  fixed_keyword_cloud_s1_v1                    tplus2  2012-09-03 05:18:37   \n",
       "\n",
       "  action                 date email_domain first_name             city state  \\\n",
       "0  click  2012-09-01 01:02:03    yahoo.com    michael           SPARTA    TN   \n",
       "1  click  2011-07-13 21:57:47  hotmail.com      Maria      Chula vista    CA   \n",
       "2  click  2012-09-01 01:24:50    yahoo.com      Frank  FORT LAUDERDALE    FL   \n",
       "3  click  2012-09-01 01:24:50    yahoo.com      Frank  FORT LAUDERDALE    FL   \n",
       "4  click  2012-09-01 01:24:50    yahoo.com      Frank  FORT LAUDERDALE    FL   \n",
       "\n",
       "     zip  degree_level hs_or_ged_year pcp_score  keyword  \\\n",
       "0  38583       Some HS           2013   0.10344  kroger    \n",
       "1  91911     Associate           1979  0.320591  costco    \n",
       "2  33301  Some College           1981  0.174869   fedex    \n",
       "3  33301  Some College           1981  0.174869   fedex    \n",
       "4  33301  Some College           1981  0.174869   fedex    \n",
       "\n",
       "                              variant_vals  \n",
       "0                 account login info s2 v1  \n",
       "1                                      nan  \n",
       "2                 account login info s2 v1  \n",
       "3  fixed keyword cloud s1 v1 tplus1 age22   \n",
       "4         fixed keyword cloud s1 v1 tplus2  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_and_members.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-f2b8fe6ac5f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m                            \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memails_and_members\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhs_or_ged_year\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                            \u001b[1;31m#pd.DataFrame(data=vX.toarray(), columns = vectorizer.get_feature_names()),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                            \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvX_variants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer_variant_vals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m                           ], axis=1)\n\u001b[0;32m      9\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\scipy\\sparse\\compressed.pyc\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[1;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m     \u001b[1;31m##############################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\scipy\\sparse\\coo.pyc\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m         \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m         \u001b[0mfortran\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfortran\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\scipy\\sparse\\base.pyc\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 793\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__numpy_ufunc__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# creating dummy values for the categorical variables\n",
    "features = pd.concat(objs=[pd.get_dummies(emails_and_members.email_type), \n",
    "                           pd.get_dummies(emails_and_members.email_variant),\n",
    "                           #pd.get_dummies(emails_and_members.degree_level),\n",
    "                           pd.get_dummies(emails_and_members.hs_or_ged_year),\n",
    "                           #pd.DataFrame(data=vX.toarray(), columns = vectorizer.get_feature_names()),\n",
    "                           pd.DataFrame(data=vX_variants.toarray(), columns = vectorizer_variant_vals.get_feature_names())\n",
    "                          ], axis=1)\n",
    "features.head()\n",
    "target = pd.get_dummies(emails_and_members.action).click"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Now building the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_performance(model):\n",
    "    total_pred = model.predict(X)\n",
    "    prod_predictions = pd.DataFrame(data=total_pred, columns=['predicted'])\n",
    "    prod_predictions['actual'] = y\n",
    "    print '\\nprediction counts\\n', prod_predictions[prod_predictions.predicted == 1].actual.value_counts()\n",
    "    print '\\ntotal predicted ', sum(prod_predictions[prod_predictions.predicted == 1].count()) \n",
    "    revenue = len(prod_predictions[(prod_predictions.predicted == 1) & (prod_predictions.actual == 1)]) * .12\n",
    "    cost =len(prod_predictions[(prod_predictions.predicted == 1)]) * .4/1000\n",
    "    print 'revenue = ', revenue\n",
    "    print 'cost = ', cost\n",
    "    print 'profit = ', revenue-cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-35b2f5884a98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# This is important\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36mas_matrix\u001b[1;34m(self, columns)\u001b[0m\n\u001b[0;32m   2233\u001b[0m         \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2234\u001b[0m         \"\"\"\n\u001b[1;32m-> 2235\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AXIS_REVERSED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2237\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2131\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2132\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2134\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   2123\u001b[0m         \u001b[1;34m\"\"\" consolidate _data. if the blocks have changed, then clear the cache \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2124\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2125\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2126\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2127\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m   2129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2131\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2132\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36mconsolidate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2832\u001b[0m         \u001b[0mbm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2833\u001b[1;33m         \u001b[0mbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2834\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2835\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2836\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2837\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2838\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2840\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   3815\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3816\u001b[0m         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n\u001b[1;32m-> 3817\u001b[1;33m                                       _can_consolidate=_can_consolidate)\n\u001b[0m\u001b[0;32m   3818\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3819\u001b[0m             \u001b[0mnew_blocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, _can_consolidate)\u001b[0m\n\u001b[0;32m   3841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3842\u001b[0m         \u001b[0margsort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3843\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3844\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = features.as_matrix().astype(int).astype(float)\n",
    "y = target\n",
    "# This is important\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "print \"Feature space holds %d observations and %d features\" % X.shape\n",
    "print \"Unique target labels:\", np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier, RidgeClassifierCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:100000], y[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "#clf = LogisticRegression(class_weight='auto')\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "predictions = clf.predict(X_test)\n",
    "print str(clf).split('(')[0], ' Classifier'\n",
    "\n",
    "print classification_report(y_test, predictions, target_names=['no click','click'])\n",
    "print confusion_matrix(y_test, predictions)\n",
    "print '\\n-------------------------\\n'\n",
    "#model_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier  Classifier\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.64      0.54      0.59    125823\n",
      "        1.0       0.55      0.65      0.59    107354\n",
      "\n",
      "avg / total       0.60      0.59      0.59    233177\n",
      "\n",
      "[[67753 58070]\n",
      " [37466 69888]]\n",
      "\n",
      "-------------------------\n",
      "\n",
      "\n",
      "prediction counts\n",
      "1    279572\n",
      "0    232846\n",
      "dtype: int64\n",
      "\n",
      "total predicted  1024836\n",
      "revenue =  33548.64\n",
      "cost =  204.9672\n",
      "profit =  33343.6728\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(class_weight='auto')\n",
    "#clf = LogisticRegression(class_weight='auto')\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "predictions = clf.predict(X_test)\n",
    "print str(clf).split('(')[0], ' Classifier'\n",
    "\n",
    "print classification_report(y_test, predictions, target_names=['no click','click'])\n",
    "print confusion_matrix(y_test, predictions)\n",
    "print '\\n-------------------------\\n'\n",
    "model_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "prediction counts\n",
      "1    277164\n",
      "0    197314\n",
      "dtype: int64\n",
      "\n",
      "total predicted  948956\n",
      "revenue =  33259.68\n",
      "cost =  189.7912\n",
      "profit =  33069.8888\n"
     ]
    }
   ],
   "source": [
    "model_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.64      0.62      0.63    126357\n",
      "        1.0       0.57      0.60      0.58    106820\n",
      "\n",
      "avg / total       0.61      0.61      0.61    233177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print 'Decision Tree Classifier'\n",
    "print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Classifier\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.65      0.61      0.63    126357\n",
      "        1.0       0.57      0.60      0.59    106820\n",
      "\n",
      "avg / total       0.61      0.61      0.61    233177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "predictions = clf.predict(X_test)\n",
    "print 'RandomForest Classifier'\n",
    "print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.66      0.54      0.60    126357\n",
      "        1.0       0.55      0.68      0.61    106820\n",
      "\n",
      "avg / total       0.61      0.60      0.60    233177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "predictions = clf.predict(X_test)\n",
    "print 'Gradient Boosting Classifier'\n",
    "print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.91      0.00      0.00    258449\n",
      "        1.0       0.29      1.00      0.46    107894\n",
      "\n",
      "avg / total       0.73      0.29      0.13    366343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "predictions = clf.predict(X_test)\n",
    "print 'Gradient Boosting Classifier'\n",
    "print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Classifier\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.71      1.00      0.83    258449\n",
      "        1.0       1.00      0.00      0.00    107894\n",
      "\n",
      "avg / total       0.79      0.71      0.58    366343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = ExtraTreesClassifier()\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "predictions = clf.predict(X_test)\n",
    "print 'Extra Trees Classifier'\n",
    "print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifier\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.71      1.00      0.83    258449\n",
      "        1.0       1.00      0.00      0.00    107894\n",
      "\n",
      "avg / total       0.79      0.71      0.58    366343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "predictions = clf.predict(X_test)\n",
    "print 'Logistic Regression Classifier'\n",
    "print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate the model (using the value K=5)\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "knn.fit(X_train, y_train.ravel())\n",
    "predictions = knn.predict(X_test)\n",
    "print 'K Nearest Neighbor Regression Classifier'\n",
    "print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "predictions = clf.predict(X_test)\n",
    "print 'Support Vector Machines Classifier'\n",
    "print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machines Classifier\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.71      1.00      0.83    258949\n",
      "        1.0       0.67      0.00      0.00    107394\n",
      "\n",
      "avg / total       0.70      0.71      0.59    366343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "predictions = clf.predict(X_test)\n",
    "print 'Support Vector Machines Classifier'\n",
    "print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Classifier\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      1.00      0.98    224300\n",
      "        1.0       0.00      0.00      0.00      8877\n",
      "\n",
      "avg / total       0.93      0.96      0.94    233177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier()\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "predictions = clf.predict(X_test)\n",
    "print 'SGD Classifier'\n",
    "print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def run_cv(X,y,clf_class,**kwargs):\n",
    "    # Construct a kfolds object\n",
    "    kf = KFold(len(y),n_folds=5,shuffle=True)\n",
    "    y_pred = y.copy()\n",
    "    # Iterate through folds\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        # Initialize a classifier with key word arguments\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def accuracy(y_true,y_pred):\n",
    "    # NumPy interprets True and False as 1. and 0.\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "print \"Random forest:\"\n",
    "print \"%.3f\" % accuracy(y, run_cv(X,y,GradientBoostingClassifier))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def run_cv(X,y,clf_class,**kwargs):\n",
    "    # Construct a kfolds object\n",
    "    kf = KFold(len(y),n_folds=5,shuffle=True)\n",
    "    y_pred = y.copy()\n",
    "    \n",
    "    # Iterate through folds\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        # Initialize a classifier with key word arguments\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "def accuracy(y_true,y_pred):\n",
    "    # NumPy interprets True and False as 1. and 0.\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "#print \"Support vector machines:\"\n",
    "#print \"%.3f\" % accuracy(y, run_cv(X,y,SVC))\n",
    "print \"Random forest:\"\n",
    "print \"%.3f\" % accuracy(y, run_cv(X,y,RF))\n",
    "print \"K-nearest-neighbors:\"\n",
    "print \"%.3f\" % accuracy(y, run_cv(X,y,KNN))\n",
    "print \"Logistic Regression:\"\n",
    "print \"%.3f\" % accuracy(y, run_cv(X,y,LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_prob_cv(X, y, clf_class, **kwargs):\n",
    "    kf = KFold(len(y), n_folds=5, shuffle=True)\n",
    "    y_prob = np.zeros((len(y),2))\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        # Predict probabilities, not classes\n",
    "        y_prob[test_index] = clf.predict_proba(X_test)\n",
    "    return y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use 10 estimators so predictions are all multiples of 0.1\n",
    "pred_prob = run_prob_cv(X, y, RF, n_estimators=10)\n",
    "pred_churn = pred_prob[:,1]\n",
    "is_churn = y == 1\n",
    "\n",
    "# Number of times a predicted probability is assigned to an observation\n",
    "counts = pd.value_counts(pred_churn)\n",
    "\n",
    "# calculate true probabilities\n",
    "true_prob = {}\n",
    "for prob in counts.index:\n",
    "    true_prob[prob] = np.mean(is_churn[pred_churn == prob])\n",
    "    true_prob = pd.Series(true_prob)\n",
    "\n",
    "# pandas-fu\n",
    "counts = pd.concat([counts,true_prob], axis=1).reset_index()\n",
    "counts.columns = ['pred_prob', 'count', 'true_prob']\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now running the predictions based on the model we created\n",
    "model = \n",
    "model.fit(X)\n",
    "predictions = model.predict(X)\n",
    "predictions.value_counts()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
